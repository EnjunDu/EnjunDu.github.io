---
title: "CARA: Concept-Aware Risk Attention for Interpretable Collision Prediction"
authors:
  - Zhishan Tao
  - Ruoyu Wang
  - Yucheng Wu
  - admin
  - Yilei Yuan
  - Sherwin Ho
  - Yue Su
  - Jinbo Su
  - "Yi Hong*"



# Author notes (optional)
author_notes:
  - ''
  - 'Corresponding Author'
date: '2025-10-05'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2025-10-05'

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ["conference"]

# Publication name and optional abbreviated publication name.
publication: 
publication_short: 

abstract: 'Collision detection in autonomous driving faces a critical interpretability challenge, as existing systems remain largely opaque in safety-critical decision-making. Current methods either rely on post-hoc explainers with limited fidelity or require costly manual annotations, failing to reconcile predictive accuracy with interpretability. To address these limitations, we propose leveraging natural language processing to extract interpretable risk concepts from real-world accident reports, bridging the semantic gap between textual accident descriptions and visual collision scenarios. We introduce CARA (Concept-Aware Risk Attention), a framework that uses language model-driven concept extraction and multimodal language-vision alignment to automatically discover risk-aware semantic concepts. Unlike traditional feature-driven attention mechanisms, CARA grounds spatial-temporal attention allocation in these human-understandable concepts derived from linguistic accident analysis. Experiments on standard benchmarks demonstrate that CARA achieves competitive accuracy and early warning capability while providing transparent, concept-based explanations for risk assessment in safety-critical AI systems.'

# Summary. An optional shortened abstract.
#summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.

tags:
  - MultiModal Learning



# links:
# - name: ""
#   url: ""
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/jdD8gXaTZsc)'
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

{{% callout note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

